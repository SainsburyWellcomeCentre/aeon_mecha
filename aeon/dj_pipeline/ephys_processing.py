import datajoint as dj
from aeon.dj_pipeline import acquisition, ephys, get_schema_name

schema = dj.schema(get_schema_name("ephys_processing"))
logger = dj.logger


@schema
class UnitQuality(dj.Lookup):
    definition = """
    unit_quality:  varchar(100)  # unit quality type - e.g. 'good', 'MUA', 'noise', etc.
    ---
    unit_quality_description='':  varchar(4000)
    """
    contents = [
        ("good", "single unit"),
        ("ok", "probably a single unit, but could be contaminated"),
        ("mua", "multi-unit activity"),
        ("noise", "bad unit"),
        ("n.a.", "not available"),
    ]


@schema
class ClusteringMethod(dj.Lookup):
    definition = """ # Method for spike sorting
    clustering_method: varchar(16)
    ---
    clustering_method_desc: varchar(1000)
    """

    contents = [
        ("kilosort2", "kilosort2 clustering method"),
        ("kilosort2.5", "kilosort2.5 clustering method"),
        ("kilosort3", "kilosort3 clustering method"),
    ]


@schema
class ClusteringParamSet(dj.Lookup):
    definition = """ # Parameter set for spike sorting
    paramset_id: varchar(16)
    ---
    -> ClusteringMethod    
    paramset_description='': varchar(1000)
    params: longblob  # dictionary of all applicable parameters
    """


@schema
class ClusteringTask(dj.Manual):
    definition = """
    # Manual table for defining a clustering task ready to be run
    -> ephys.EphysBlockProcessing
    -> ClusteringParamSet
    """


@schema
class PreProcessing(dj.Computed):
    definition = """
    -> ephys.ClusteringTask
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    clustering_output_dir: varchar(255)  #  clustering output directory relative to the clustering root data directory
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the pre-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Pre-processing the ephys data, before spike sorting. E.g.:
        - data concatenation
        - bad channel detection/removal
        - band-pass filtering
        - common average referencing
        - etc.
        """
        pass


@schema
class SpikeSorting(dj.Computed):
    definition = """
    -> PreProcessing
    ---
    execution_time: datetime        # datetime of the start of this step
    execution_duration: float       # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the spike sorting step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Spike sorting the ephys data. E.g.:
        - kilosort2
        - kilosort2.5
        - kilosort3
        """
        pass


@schema
class PostProcessing(dj.Computed):
    definition = """
    -> SpikeSorting
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the post-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Post-processing the ephys data, after spike sorting. E.g.:
        - quality assessment
        - waveform extraction
        - etc.
        """
        pass


@schema
class Clustering(dj.Imported):
    definition = """
    -> PostProcessing
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class Unit(dj.Part):
        definition = """  # Identified units and their properties
        -> master
        unit: int
        ---
        -> ephys.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit
        -> UnitQuality
        spike_count: int         # how many spikes in this recording for this unit
        spike_times: longblob    # (s) array of spike times of this unit, relative to the start of the EphysBlock (in native clock)
        spike_sites : longblob   # array of electrode associated with each spike
        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe    
        """

    def make(self, key):
        """
        From clustering output, extract units, spike times, electrode, etc.
        Also, synchronize the spike times to the HARP clock.
        """
        pass


@schema
class Waveform(dj.Imported):
    definition = """
    # A set of spike waveforms for units out of a given Clustering
    -> Clustering
    """

    class UnitWaveform(dj.Part):
        definition = """
        # Representative waveform for a given unit
        -> master
        -> Clustering.Unit
        ---
        unit_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode
        """

    class ChannelWaveform(dj.Part):
        definition = """
        # Spike waveforms and their mean across spikes for the given unit at the given electrode
        -> master
        -> Clustering.Unit
        -> ephys.ElectrodeConfig.Electrode  
        --- 
        channel_waveform: longblob   # (uV) mean waveform across spikes of the given unit at the given electrode
        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit
        """
        
    def make(self, key):
        """
        Extract waveforms for each unit and electrode.
        """
        pass


@schema
class QualityMetric(dj.Lookup):
    definition = """
    quality_metric: varchar(100)  # quality metric type - e.g. 'isi_violation', 'amplitude_violation', etc.
    ---
    quality_metric_description='':  varchar(4000)
    """


@schema
class ClusteringQuality(dj.Imported):
    definition = """
    -> Clustering
    """
    
    class Metric(dj.Part):
        definition = """  # Quality metrics for a given unit
        -> master
        -> Clustering.Unit
        -> QualityMetric
        ---
        metric_value: varchar(100)  # value of the quality metric
        """
    

@schema
class SyncedSpikeTimes(dj.Imported):
    definition = """
    -> Clustering
    """

    class Unit(dj.Part):
        definition = """
        -> master
        -> Clustering.Unit
        ---
        spike_times: longblob  # (s) synchronized spike times (i.e. in HARP clock)
        """

    def make(self, key):
        """
        Synchronize the spike times to the HARP clock.
        """
        pass
