import datajoint as dj
from aeon.dj_pipeline import acquisition, ephys, get_schema_name

schema = dj.schema(get_schema_name("ephys_processing"))
logger = dj.logger



@schema
class ElectrodeGroup(dj.Manual):
    """
    A group of electrodes that are used for spike sorting.
    All or subset of the electrodes from a particular electrode configuration 
    """
    definition = """
    -> ephys.ElectrodeConfig
    electrode_group: varchar(16)  # e.g. "all", "shank1", etc.
    ---
    electrode_group_description: varchar(1000) 
    electrode_count: int
    """

    class Electrode(dj.Part):
        definition = """  # Individual electrode in the group
        -> master
        -> ephys.ElectrodeConfig.Electrode
        """


@schema
class UnitQuality(dj.Lookup):
    definition = """
    unit_quality:  varchar(100)  # unit quality type - e.g. 'good', 'MUA', 'noise', etc.
    ---
    unit_quality_description='':  varchar(4000)
    """
    contents = [
        ("good", "single unit"),
        ("ok", "probably a single unit, but could be contaminated"),
        ("mua", "multi-unit activity"),
        ("noise", "bad unit"),
        ("n.a.", "not available"),
    ]


@schema
class SortingMethod(dj.Lookup):
    definition = """ # Method for spike sorting
    sorting_method: varchar(16)
    ---
    sorting_method_desc: varchar(1000)
    """

    contents = [
        ("kilosort2", "kilosort2 sorting method"),
        ("kilosort2.5", "kilosort2.5 sorting method"),
        ("kilosort3", "kilosort3 sorting method"),
    ]


@schema
class SortingParamSet(dj.Lookup):
    definition = """ # Parameter set for spike sorting
    paramset_id: varchar(16)
    ---
    -> SortingMethod    
    paramset_description='': varchar(1000)
    params: longblob  # dictionary of all applicable parameters
    """


@schema
class SortingTask(dj.Manual):
    """
    A manual table for defining a sorting task ready to be run.
    A sorting task is defined by a unique combination of:
    - ephys block
    - electrode group
    - sorting parameter set
    """
    definition = """
    # Manual table for defining a sorting task ready to be run
    -> ephys.EphysBlock
    -> ElectrodeGroup
    -> SortingParamSet
    """


@schema
class PreProcessing(dj.Computed):
    definition = """
    -> ephys.SortingTask
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    sorting_output_dir: varchar(255)  #  sorting output directory relative to the sorting root data directory
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the pre-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Pre-processing the ephys data, before spike sorting. E.g.:
        - data concatenation (based on EphysBlock)
        - channel selection (based on ElectrodeGroup)
        - bad channel detection/removal
        - band-pass filtering
        - common average referencing
        - etc.
        """
        pass


@schema
class SpikeSorting(dj.Computed):
    definition = """
    -> PreProcessing
    ---
    execution_time: datetime        # datetime of the start of this step
    execution_duration: float       # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the spike sorting step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Spike sorting the ephys data. E.g.:
        - kilosort2
        - kilosort2.5
        - kilosort3
        """
        pass


@schema
class PostProcessing(dj.Computed):
    definition = """
    -> SpikeSorting
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the post-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@ephys-processed
        """

    def make(self, key):
        """ Post-processing the ephys data, after spike sorting. E.g.:
        - quality assessment
        - waveform extraction
        - etc.
        """
        pass


@schema
class SortedSpikes(dj.Imported):
    definition = """
    -> PostProcessing
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class Unit(dj.Part):
        definition = """  # Identified units and their properties
        -> master
        unit: int
        ---
        -> ephys.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit
        -> UnitQuality
        spike_count: int         # how many spikes in this recording for this unit
        spike_times: longblob    # (s) array of spike times of this unit, relative to the start of the EphysBlock (in native clock)
        spike_sites : longblob   # array of electrode associated with each spike
        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe    
        """

    def make(self, key):
        """
        From sorted spikes output, extract units, spike times, electrode, etc.
        Also, synchronize the spike times to the HARP clock.
        """
        pass


@schema
class Waveform(dj.Imported):
    definition = """
    # A set of spike waveforms for units out of a given SortedSpikes
    -> SortedSpikes
    """

    class UnitWaveform(dj.Part):
        definition = """
        # Representative waveform for a given unit
        -> master
        -> SortedSpikes.Unit
        ---
        unit_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode
        """

    class ChannelWaveform(dj.Part):
        definition = """
        # Spike waveforms and their mean across spikes for the given unit at the given electrode
        -> master
        -> SortedSpikes.Unit
        -> ephys.ElectrodeConfig.Electrode  
        --- 
        channel_waveform: longblob   # (uV) mean waveform across spikes of the given unit at the given electrode
        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit
        """
        
    def make(self, key):
        """
        Extract waveforms for each unit and electrode.
        """
        pass


@schema
class QualityMetric(dj.Lookup):
    definition = """
    quality_metric: varchar(100)  # quality metric type - e.g. 'isi_violation', 'amplitude_violation', etc.
    ---
    quality_metric_description='':  varchar(4000)
    """


@schema
class SortingQuality(dj.Imported):
    definition = """
    -> SortedSpikes
    """
    
    class Metric(dj.Part):
        definition = """  # Quality metrics for a given unit
        -> master
        -> SortedSpikes.Unit
        -> QualityMetric
        ---
        metric_value: varchar(100)  # value of the quality metric
        """
    

@schema
class SyncedSpikes(dj.Imported):
    definition = """
    -> SortedSpikes
    """

    class Unit(dj.Part):
        definition = """
        -> master
        -> SortedSpikes.Unit
        ---
        spike_times: longblob  # (s) synchronized spike times (i.e. in HARP clock)
        """

    def make(self, key):
        """
        Synchronize the spike times to the HARP clock.
        """
        pass
