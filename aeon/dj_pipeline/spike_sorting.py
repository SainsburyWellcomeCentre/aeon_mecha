import datajoint as dj
import numpy as np
from pathlib import Path
import pandas as pd
import json
from datetime import datetime, UTC

from aeon.dj_pipeline import acquisition, ephys, get_schema_name
from aeon.dj_pipeline.utils.paths import get_sorting_root_dir

schema = dj.schema(get_schema_name("spike_sorting"))
logger = dj.logger


@schema
class ElectrodeGroup(dj.Manual):
    """
    A group of electrodes that are used for spike sorting.
    All or subset of the electrodes from a particular electrode configuration 
    """
    definition = """
    -> ephys.ElectrodeConfig
    electrode_group: varchar(16)  # e.g. 'all', 'shank1', etc.
    ---
    electrode_group_description: varchar(1000) 
    electrode_count: int
    """

    class Electrode(dj.Part):
        definition = """  # Individual electrode in the group
        -> master
        -> ephys.ElectrodeConfig.Electrode
        """


@schema
class UnitQuality(dj.Lookup):
    definition = """
    unit_quality:  varchar(100)  # unit quality type - e.g. 'good', 'MUA', 'noise', etc.
    ---
    unit_quality_description='':  varchar(4000)
    """
    contents = [
        ("good", "single unit"),
        ("ok", "probably a single unit, but could be contaminated"),
        ("mua", "multi-unit activity"),
        ("noise", "bad unit"),
        ("n.a.", "not available"),
    ]


@schema
class SortingMethod(dj.Lookup):
    definition = """ # Method for spike sorting
    sorting_method: varchar(16)
    ---
    sorting_method_desc: varchar(1000)
    """

    contents = [
        ("kilosort2", "kilosort2 sorting method"),
        ("kilosort2.5", "kilosort2.5 sorting method"),
        ("kilosort3", "kilosort3 sorting method"),
        ("kilosort4", "kilosort4 sorting method")
    ]


@schema
class SortingParamSet(dj.Lookup):
    definition = """ # Parameter set for spike sorting
    paramset_id: varchar(16)
    ---
    -> SortingMethod    
    paramset_description='': varchar(1000)
    params: longblob  # dictionary of all applicable parameters
    """


@schema
class SortingTask(dj.Manual):
    """
    A manual table for defining a sorting task ready to be run.
    A sorting task is defined by a unique combination of:
    - ephys block
    - electrode group
    - sorting parameter set
    """
    definition = """
    # Manual table for defining a sorting task ready to be run
    -> ephys.EphysBlock
    -> ElectrodeGroup
    -> SortingParamSet
    """


@schema
class PreProcessing(dj.Computed):
    definition = """
    -> SortingTask
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    sorting_output_dir: varchar(255)  #  sorting output directory relative to the sorting root data directory
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the pre-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@dj_store
        """

    @classmethod
    def infer_output_dir(cls, key, relative=False, mkdir=False):
        sorting_root_dir = get_sorting_root_dir()
        # Infer output directory
        sorting_method = (SortingMethod * SortingParamSet & key).fetch1("sorting_method")
        start_str = key["block_start"].strftime("%Y-%m-%dT%H-%M-%S")
        end_str = key["block_end"].strftime("%Y-%m-%dT%H-%M-%S")

        output_dir = (sorting_root_dir / key["experiment_name"] / "ephys_blocks"
                      / f"{start_str}_{end_str}"
                      / key["electrode_group"]
                      / f"{sorting_method}_{key['paramset_id']}")

        if mkdir:
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"{output_dir} created!")

        return output_dir.relative_to(sorting_root_dir) if relative else output_dir

    def make_fetch(self, key):
        # Infer output directory
        output_dir = self.infer_output_dir(key, mkdir=True)
        recording_dir = output_dir.parent / "recording"
        recording_dir.mkdir(parents=True, exist_ok=True)
        recording_file = recording_dir / "si_recording.pkl"

        # Load ephys data
        ephys_files, dir_types = (
            ephys.EphysChunk.File & (ephys.EphysBlockInfo.Chunk & key)
            & "file_name LIKE '%AmplifierData%.bin'"
        ).fetch("file_path", "directory_type", order_by="chunk_start")

        # Channels
        electrodes_df = (
            (
                ephys.EphysBlockInfo.Channel
                * ephys.ElectrodeConfig.Electrode
                * ElectrodeGroup.Electrode
                * ephys.ProbeType.Electrode
                & key
            )
            .fetch(format="frame")
            .reset_index()
        )

        num_channels = len(ephys.ElectrodeConfig.Electrode & key)
        # Neuropixels 2.0 constants - TODO: read this from the metadata
        fs_hz = 30e3
        gain_to_uV = 3.05176
        offset_to_uV = -2048 * gain_to_uV

        return ephys_files, dir_types, electrodes_df, num_channels, fs_hz, gain_to_uV, offset_to_uV, output_dir, recording_file, recording_dir

    def make_compute(self, key, ephys_files, dir_types, electrodes_df,
             num_channels, fs_hz, gain_to_uV, offset_to_uV,
             output_dir, recording_file, recording_dir):
        """ Pre-processing the ephys data, before spike sorting. E.g.:
        - data concatenation (based on EphysBlock)
        - channel selection (based on ElectrodeGroup)
        - bad channel detection/removal
        - band-pass filtering
        - common average referencing
        - etc.
        """
        import probeinterface as pi
        import spikeinterface as si
        import spikeinterface.extractors as se

        execution_time = datetime.now(UTC)

        # Concatenate recordings
        si_recs = []
        for f, d in zip(ephys_files, dir_types):
            ephys_dir = acquisition.Experiment.get_data_directory(key, directory_type=d)
            si_rec = se.read_binary(
                ephys_dir / f,
                sampling_frequency=fs_hz,
                dtype=np.uint16,
                num_channels=num_channels,
                gain_to_uV=gain_to_uV,
                offset_to_uV=offset_to_uV)
            si_recs.append(si_rec)
        si_recording = si.concatenate_recordings(si_recs)

        # Select channels based on the electrode group
        in_use_chn_ids = si_recording.channel_ids[electrodes_df.channel_idx.values]
        chn2remove = set(si_recording.channel_ids) - set(in_use_chn_ids)
        si_recording = si_recording.remove_channels(list(chn2remove))
        in_use_chn_ind = [si_recording.channel_ids.tolist().index(chn_id) for chn_id in in_use_chn_ids]
        electrodes_df.channel_idx = in_use_chn_ind

        # Create SI probe object
        probe_df = electrodes_df.copy()
        probe_df.rename(
            columns={
                "electrode": "contact_ids",
                "shank": "shank_ids",
                "x_coord": "x",
                "y_coord": "y",
            },
            inplace=True,
        )
        probe_df["contact_shapes"] = "circle"
        probe_df["radius"] = 10
        si_probe = pi.Probe.from_dataframe(probe_df)

        si_probe.set_device_channel_indices(electrodes_df["channel_idx"].values)
        si_recording.set_probe(probe=si_probe, in_place=True)

        # Run preprocessing and save results to output folder
        si_recording = ephys_preproc(si_recording)
        si_recording.dump_to_pickle(file_path=recording_file, relative_to=output_dir)

        return output_dir, execution_time, recording_dir

    def make_insert(self, key, output_dir, execution_time, recording_dir):
        self.insert1(
            {
                **key,
                "sorting_output_dir": output_dir.relative_to(get_sorting_root_dir()).as_posix(),
                "execution_time": execution_time,
                "execution_duration": (
                    datetime.now(UTC) - execution_time
                ).total_seconds()
                / 3600,
            }
        )
        # Insert result files
        self.File.insert(
            [
                {**key, "file_name": f.relative_to(output_dir.parent).as_posix(), "file": f}
                for f in recording_dir.rglob("*")
                if f.is_file()
            ]
        )


@schema
class SpikeSorting(dj.Computed):
    definition = """
    -> PreProcessing
    ---
    execution_time: datetime        # datetime of the start of this step
    execution_duration: float       # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the spike sorting step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@dj_store
        """

    def make_fetch(self, key):
        sorting_root_dir = get_sorting_root_dir()
        # Load recording object.
        sorting_method, params = (
                SortingTask * SortingParamSet & key
        ).fetch1("sorting_method", "params")
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        recording_file = (PreProcessing.File
                          & key & "file_name LIKE '%si_recording.pkl'").fetch1("file")

        return recording_file, output_dir, params, sorting_method

    def make_compute(self, key, recording_file, output_dir, params, sorting_method):
        import spikeinterface as si
        from spikeinterface import sorters

        execution_time = datetime.now(UTC)

        sorter_name = sorting_method.replace(".", "_")
        sorting_output_dir = output_dir / "spike_sorting"

        """ Run spike sorting using the specified method and parameters. """
        si_recording: si.BaseRecording = si.load(
            recording_file, base_folder=output_dir
        )
        sorting_params = params["SI_SORTING_PARAMS"]

        si_sorting: si.sorters.BaseSorter = si.sorters.run_sorter(
            sorter_name=sorter_name,
            recording=si_recording,
            folder=sorting_output_dir,
            remove_existing_folder=True,
            verbose=True,
            singularity_image=sorter_name not in si.sorters.installed_sorters(),
            **sorting_params,
        )

        # Save sorting object
        sorting_save_path = sorting_output_dir / "si_sorting.pkl"
        si_sorting.dump_to_pickle(sorting_save_path, relative_to=output_dir)

        execution_duration = (datetime.now(UTC) - execution_time).total_seconds() / 3600

        return sorting_output_dir, execution_time, execution_duration

    def make_insert(self, key, sorting_output_dir, execution_time, execution_duration):
        self.insert1(
            {
                **key,
                "execution_time": execution_time,
                "execution_duration": execution_duration
            }
        )
        # Insert result files
        self.File.insert(
            [
                {**key, "file_name": f.relative_to(sorting_output_dir).as_posix(), "file": f}
                for f in sorting_output_dir.rglob("*")
                if f.is_file()
            ]
        )


@schema
class PostProcessing(dj.Computed):
    definition = """
    -> SpikeSorting
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the post-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@dj_store
        """

    def make_fetch(self, key):
        sorting_root_dir = get_sorting_root_dir()
        # Load recording object.
        sorting_method, params = (
                SortingTask * SortingParamSet & key
        ).fetch1("sorting_method", "params")
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        recording_file = (PreProcessing.File
                          & key & "file_name LIKE '%si_recording.pkl'").fetch1("file")
        sorting_file = (SpikeSorting.File
                        & key & "file_name LIKE '%si_sorting.pkl'").fetch1("file")

        return recording_file, sorting_file, output_dir, params

    def make_compute(self, key, recording_file, sorting_file, output_dir, params):
        """ Post-processing the ephys data, after spike sorting. E.g.:
        - quality assessment
        - waveform extraction
        - etc.
        """
        import spikeinterface as si
        from spikeinterface import sorters

        execution_time = datetime.now(UTC)

        si_recording: si.BaseRecording = si.load(
            recording_file, base_folder=output_dir
        )
        si_sorting: si.sorters.BaseSorter = si.load(
            sorting_file, base_folder=output_dir
        )

        postprocessing_params = params["SI_POSTPROCESSING_PARAMS"]

        job_kwargs = postprocessing_params.get(
            "job_kwargs", {"n_jobs": -1, "chunk_duration": "1s"}
        )

        analyzer_output_dir = output_dir / "sorting_analyzer"

        has_units = si_sorting.unit_ids.size > 0

        # ---- run the analyzer extensions ----
        if not has_units:
            logger.info("No units found in sorting object. Skipping sorting analyzer.")
            analyzer_output_dir.mkdir(parents=True, exist_ok=True)  # create empty directory anyway, for consistency
            return

        # Sorting Analyzer
        sorting_analyzer = si.create_sorting_analyzer(
            sorting=si_sorting,
            recording=si_recording,
            format="binary_folder",
            folder=analyzer_output_dir,
            sparse=True,
            overwrite=True,
        )

        # The order of extension computation is drawn from sorting_analyzer.get_computable_extensions()
        # each extension is parameterized by params specified in extensions_params dictionary (skip if not specified)
        extensions_params = postprocessing_params.get("extensions", {})
        extensions_to_compute = {
            ext_name: extensions_params[ext_name]
            for ext_name in sorting_analyzer.get_computable_extensions()
            if ext_name in extensions_params
        }

        sorting_analyzer.compute(extensions_to_compute, **job_kwargs)

        execution_duration = (datetime.now(UTC) - execution_time).total_seconds() / 3600

        return analyzer_output_dir, execution_time, execution_duration

    def make_insert(self, key, analyzer_output_dir, execution_time, execution_duration):
        self.insert1(
            {
                **key,
                "execution_time": execution_time,
                "execution_duration": execution_duration,
            }
        )
        self.File.insert(
            [
                {**key, "file_name": f.relative_to(analyzer_output_dir).as_posix(), "file": f}
                for f in analyzer_output_dir.rglob("*")
                if f.is_file()
            ]
        )


@schema
class SIExport(dj.Computed):
    definition = """
    -> PostProcessing
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class File(dj.Part):
        definition = """  # File(s) generated by the post-processing step
        -> master
        file_name: varchar(255)
        ---
        file: filepath@dj_store
        """

    def make(self, key):
        import spikeinterface as si
        from spikeinterface import exporters

        execution_time = datetime.now(UTC)

        sorting_root_dir = get_sorting_root_dir()
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        analyzer_output_dir = output_dir / "sorting_analyzer"
        sorting_analyzer = si.load_sorting_analyzer(folder=analyzer_output_dir)

        job_kwargs = {"n_jobs": 0.8, "chunk_duration": "1s"}
        si.exporters.export_report(
            sorting_analyzer=sorting_analyzer,
            output_folder=analyzer_output_dir / "spikeinterface_report",
            remove_if_exists=True,
            **job_kwargs,
        )

        execution_duration = (datetime.now(UTC) - execution_time).total_seconds() / 3600
        self.insert1(
            {
                **key,
                "execution_time": execution_time,
                "execution_duration": execution_duration,
            }
        )
        # Insert result files
        self.File.insert(
            [
                {
                    **key,
                    "file_name": f.relative_to(analyzer_output_dir).as_posix(),
                    "file": f,
                }
                for f in (analyzer_output_dir / "spikeinterface_report").rglob("*")
                if f.is_file()
            ]
        )


@schema
class SortedSpikes(dj.Imported):
    definition = """
    -> PostProcessing
    ---
    execution_time: datetime   # datetime of the start of this step
    execution_duration: float  # execution duration in hours
    """

    class Unit(dj.Part):
        definition = """  # Identified units and their properties
        -> master
        unit: int
        ---
        -> ephys.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit
        -> UnitQuality
        spike_count: int         # how many spikes in this recording for this unit
        spike_indices: longblob  # array of spike indices into the concatenated binary data (from preprocessing)
        spike_sites : longblob   # array of electrode associated with each spike
        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe    
        """

    def make(self, key):
        """
        From sorted spikes output, extract units, spike times, electrode, etc.
        Also, synchronize the spike times to the HARP clock.
        """
        import spikeinterface as si
        from spikeinterface import sorters

        execution_time = datetime.now(UTC)

        sorting_root_dir = get_sorting_root_dir()
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        # Get channel and electrode-site mapping
        electrode_query = (
                ephys.EphysBlockInfo.Channel.proj(..., "-channel_name")
                * ephys.ElectrodeConfig.Electrode
                * ElectrodeGroup.Electrode
                * ephys.ProbeType.Electrode.proj("electrode_name")
                & key
        )

        # Get sorter method and create output directory.
        analyzer_output_dir = output_dir / "sorting_analyzer"

        sorting_file = output_dir / "spike_sorting" / "si_sorting.pkl"
        si_sorting_: si.sorters.BaseSorter = si.load(
            sorting_file, base_folder=output_dir
        )

        self.insert1(
            {
                **key,
                "execution_time": execution_time,
                "execution_duration": (datetime.now(UTC) - execution_time).total_seconds() / 3600,
            }
        )
        if si_sorting_.unit_ids.size == 0:
            logger.info(
                f"No units found in {sorting_file}. Skipping Unit ingestion..."
            )
            return

        sorting_analyzer = si.load_sorting_analyzer(folder=analyzer_output_dir)
        si_sorting = sorting_analyzer.sorting

        # Find representative channel for each unit
        unit_peak_channel: dict[int, np.ndarray] = (
            si.ChannelSparsity.from_best_channels(
                sorting_analyzer,
                1,
            ).unit_id_to_channel_indices
        )
        unit_peak_channel: dict[int, int] = {
            u: chn[0] for u, chn in unit_peak_channel.items()
        }

        spike_count_dict: dict[int, int] = si_sorting.count_num_spikes_per_unit()
        # {unit: spike_count}

        # create channel2electrode_map
        electrode_map: dict[int, dict] = {
            elec["electrode"]: elec for elec in electrode_query.fetch(as_dict=True)
        }
        channel2electrode_map = {
            chn_idx: electrode_map[int(elec_id)]
            for chn_idx, elec_id in zip(sorting_analyzer.get_probe().device_channel_indices,
                                        sorting_analyzer.get_probe().contact_ids)
        }

        # Get unit id to quality label mapping
        cluster_quality_label_map = {
            int(unit_id): (
                si_sorting.get_unit_property(unit_id, "KSLabel")
                if "KSLabel" in si_sorting.get_property_keys()
                else "n.a."
            )
            for unit_id in si_sorting.unit_ids
        }

        spike_locations = sorting_analyzer.get_extension("spike_locations")
        extremum_channel_inds = si.template_tools.get_template_extremum_channel(
            sorting_analyzer, outputs="index"
        )
        spikes_df = pd.DataFrame(
            sorting_analyzer.sorting.to_spike_vector(
                extremum_channel_inds=extremum_channel_inds
            )
        )
        for unit_idx, unit_id in enumerate(si_sorting.unit_ids):
            unit_id = int(unit_id)
            unit_spikes_df = spikes_df[spikes_df.unit_index == unit_idx]
            spike_sites = np.array(
                [
                    channel2electrode_map[chn_idx]["electrode"]
                    for chn_idx in unit_spikes_df.channel_index
                ]
            )
            unit_spikes_loc = spike_locations.get_data()[unit_spikes_df.index]
            _, spike_depths = zip(*unit_spikes_loc)  # x-coordinates, y-coordinates
            spike_indices = si_sorting.get_unit_spike_train(unit_id)

            assert len(spike_indices) == len(spike_sites) == len(spike_depths)

            self.Unit.insert1(
                {
                    **key,
                    **channel2electrode_map[unit_peak_channel[unit_id]],
                    "unit": unit_id,
                    "unit_quality": cluster_quality_label_map[unit_id],
                    "spike_indices": spike_indices,
                    "spike_count": spike_count_dict[unit_id],
                    "spike_sites": spike_sites,
                    "spike_depths": spike_depths,
                }, ignore_extra_fields=True
            )


@schema
class Waveform(dj.Imported):
    definition = """
    # A set of spike waveforms for units out of a given SortedSpikes
    -> SortedSpikes
    """

    class UnitWaveform(dj.Part):
        definition = """
        # Representative waveform for a given unit
        -> master
        -> SortedSpikes.Unit
        ---
        unit_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode
        """

    class ChannelWaveform(dj.Part):
        definition = """
        # Spike waveforms and their mean across spikes for the given unit at the given electrode
        -> master
        -> SortedSpikes.Unit
        -> ephys.ElectrodeConfig.Electrode  
        --- 
        channel_waveform: longblob   # (uV) mean waveform across spikes of the given unit at the given electrode
        """
        
    def make(self, key):
        import spikeinterface as si

        sorting_root_dir = get_sorting_root_dir()
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        # Get channel and electrode-site mapping
        electrode_query = (
                ephys.EphysBlockInfo.Channel.proj(..., "-channel_name")
                * ephys.ElectrodeConfig.Electrode
                * ElectrodeGroup.Electrode
                * ephys.ProbeType.Electrode.proj("electrode_name")
                & key
        )

        # Get sorter method and create output directory.
        analyzer_output_dir = output_dir / "sorting_analyzer"
        sorting_analyzer = si.load_sorting_analyzer(folder=analyzer_output_dir)

        self.insert1(key)

        # Find representative channel for each unit
        unit_peak_channel: dict[int, np.ndarray] = (
            si.ChannelSparsity.from_best_channels(
                sorting_analyzer, 1
            ).unit_id_to_channel_indices
        )  # {unit: peak_channel_index}
        unit_peak_channel = {u: chn[0] for u, chn in unit_peak_channel.items()}

        # create channel2electrode_map
        electrode_map: dict[int, dict] = {
            elec["electrode"]: elec for elec in electrode_query.fetch(as_dict=True)
        }
        channel2electrode_map = {
            chn_idx: electrode_map[int(elec_id)]
            for chn_idx, elec_id in zip(sorting_analyzer.get_probe().device_channel_indices,
                                        sorting_analyzer.get_probe().contact_ids)
        }

        templates = sorting_analyzer.get_extension("templates")

        for unit in (SortedSpikes.Unit & key).fetch(
                "KEY", order_by="unit"
        ):
            # Get mean waveform for this unit from all channels - (sample x channel)
            unit_waveforms = templates.get_unit_template(
                unit_id=unit["unit"], operator="average"
            )
            unit_peak_waveform = {
                **unit,
                "unit_waveform": unit_waveforms[
                                           :, unit_peak_channel[unit["unit"]]
                                           ],
            }

            unit_electrode_waveforms = [
                {
                    **unit,
                    **channel2electrode_map[chn_idx],
                    "channel_waveform": unit_waveforms[:, chn_idx],
                }
                for chn_idx in channel2electrode_map
            ]

            self.UnitWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)
            self.ChannelWaveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)


@schema
class SortingQuality(dj.Imported):
    definition = """
    -> SortedSpikes
    """
    
    class Metric(dj.Part):
        definition = """  # Quality metrics for a given unit
        -> master
        -> SortedSpikes.Unit
        ---
        qc_metrics: JSON
        """

    def make(self, key):
        import spikeinterface as si

        sorting_root_dir = get_sorting_root_dir()
        output_dir = sorting_root_dir / (PreProcessing & key).fetch1("sorting_output_dir")

        # Get sorter method and create output directory.
        analyzer_output_dir = output_dir / "sorting_analyzer"
        sorting_analyzer = si.load_sorting_analyzer(folder=analyzer_output_dir)

        self.insert1(key)

        qc_metrics = sorting_analyzer.get_extension("quality_metrics").get_data()
        template_metrics = sorting_analyzer.get_extension(
            "template_metrics"
        ).get_data()
        metrics_df = pd.concat([qc_metrics, template_metrics], axis=1)

        for unit_key in (SortedSpikes.Unit & key).fetch("KEY"):
            unit_qc = metrics_df.loc[unit_key["unit"]].to_dict()
            self.Metric.insert1(
                {**unit_key, "qc_metrics": json.dumps(unit_qc, default=str)}
            )


@schema
class SyncedSpikes(dj.Imported):
    definition = """
    -> SortedSpikes
    """

    class Unit(dj.Part):
        definition = """
        -> master
        -> SortedSpikes.Unit
        -> ephys.EphysChunk
        ---
        -> ephys.ElectrodeConfig.Electrode  # electrode associated with this unit
        spike_times: longblob  # (s) synchronized spike times (i.e. in HARP clock) for the respective EphysChunk
        """

    def make(self, key):
        """
        Synchronize the spike times to the HARP clock.
        """
        pass


# ---- Ephys preprocessing with spike interface ----

def ephys_preproc(recording):
    """
    Basic ephys preprocessing using SpikeInterface.
    1. Bandpass filter the recording between 300 Hz and 6000 Hz.
    2. Common average reference the recording using median.
    """
    import spikeinterface as si
    from spikeinterface import preprocessing

    recording = si.preprocessing.unsigned_to_signed(recording)
    recording = si.preprocessing.bandpass_filter(
        recording=recording, freq_min=300, freq_max=6000
    )
    recording = si.preprocessing.common_reference(
        recording=recording, operator="median"
    )
    return recording
