{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(target-dj-compute-behav-bouts)=\n",
    "# DataJoint pipeline: Computing behavioural bouts\n",
    "\n",
    ":::{important}\n",
    "This guide assumes you have a [DataJoint pipeline deployed](target-dj-pipeline-deployment) with [data already ingested](target-dj-data-ingestion-processing).\n",
    ":::\n",
    "\n",
    "Using position data from the [Aeon DataJoint pipeline](target-aeon-dj-pipeline), this guide walks through computing foraging, drinking, and sleeping bouts for each subject.\n",
    "\n",
    "You can also run this notebook online at [`works.datajoint.com`](https://works.datajoint.com/) using the following credentials:\n",
    " - Username: aeondemo\n",
    " - Password: aeon_djworks \n",
    "\n",
    "To access it, go to the Notebook tab at the top and in the File Browser on the left, navigate to `ucl-swc_aeon > docs > examples`, where this notebook `dj_compute_bouts.ipynb` is located.\n",
    "\n",
    ":::{note}\n",
    "The examples here use the [social0.2-aeon4](target-full-datasets) dataset. \n",
    "If you are using a different dataset, be sure to replace the experiment name and  parameters in the code below accordingly.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-28 15:29:59,542][INFO]: DataJoint 0.14.6 connected to chuan@aeon-db2:3306\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import statsmodels.api as sm\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datajoint as dj\n",
    "from aeon.dj_pipeline.analysis.block_analysis import *\n",
    "from aeon.dj_pipeline import acquisition, streams, subject\n",
    "from swc.aeon.io import api as aeon_api\n",
    "from aeon.schema.schemas import social02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_swaps(pos_df: pd.DataFrame, max_speed: float) -> pd.DataFrame:\n",
    "    \"\"\"Detect and correct swaps in the position data.\n",
    "\n",
    "    Args:\n",
    "        pos_df (pd.DataFrame): DataFrame containing position data of a single subject.\n",
    "        max_speed (float): Maximum speed (px/s) threshold over which we assume a swap.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with swaps corrected.\n",
    "    \"\"\"\n",
    "    dt = pos_df.index.diff().total_seconds()\n",
    "    dx = pos_df[\"x\"].diff()\n",
    "    dy = pos_df[\"y\"].diff()\n",
    "    pos_df[\"inst_speed\"] = np.sqrt(dx**2 + dy**2) / dt\n",
    "\n",
    "    # Identify jumps\n",
    "    jumps = (pos_df[\"inst_speed\"] > max_speed)\n",
    "    shift_down = jumps.shift(1)\n",
    "    shift_down.iloc[0] = False\n",
    "    shift_up = jumps.shift(-1)\n",
    "    shift_up.iloc[len(jumps) - 1] = False\n",
    "    jump_starts = jumps & ~shift_down\n",
    "    jump_ends = jumps & ~shift_up\n",
    "    jump_start_indices = np.where(jump_starts)[0]\n",
    "    jump_end_indices = np.where(jump_ends)[0]\n",
    "\n",
    "    if np.any(jumps):\n",
    "        # Ensure the lengths match\n",
    "        if len(jump_start_indices) > len(jump_end_indices):  # jump-in-progress at start\n",
    "            jump_end_indices = np.append(jump_end_indices, len(pos_df) - 1)\n",
    "        elif len(jump_start_indices) < len(jump_end_indices):  # jump-in-progress at end\n",
    "            jump_start_indices = np.insert(jump_start_indices, 0, 0)\n",
    "        # Remove jumps by setting speed to nan in jump regions and dropping nans\n",
    "        for start, end in zip(jump_start_indices, jump_end_indices, strict=True):\n",
    "            pos_df.loc[pos_df.index[start]:pos_df.index[end], \"inst_speed\"] = np.nan\n",
    "        pos_df.dropna(subset=[\"inst_speed\"], inplace=True)\n",
    "\n",
    "    return pos_df\n",
    "\n",
    "\n",
    "def ensure_ts_arr_datetime(array):\n",
    "    \"\"\"Ensure array is a numpy array of datetime64[ns] type.\"\"\"\n",
    "    if len(array) == 0:\n",
    "        return np.array([], dtype=\"datetime64[ns]\")\n",
    "    else:\n",
    "        return np.array(array, dtype=\"datetime64[ns]\")\n",
    "\n",
    "\n",
    "def concat_and_reorder(dfs):\n",
    "    \"\"\"Concatenate dataframes and reorder columns.\"\"\"\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    first_cols = [\"experiment_name\", \"period\"]\n",
    "    return df[first_cols + [col for col in df.columns if col not in first_cols]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2px = 5.2  # 1 cm = 5.2 px roughly for top camera\n",
    "exp = {\n",
    "    \"name\": \"social0.2-aeon4\",\n",
    "    \"presocial_start\": \"2024-01-31 11:00:00\",\n",
    "    \"presocial_end\": \"2024-02-08 15:00:00\",\n",
    "    \"social_start\": \"2024-02-09 17:00:00\",\n",
    "    \"social_end\": \"2024-02-23 12:00:00\",\n",
    "    \"postsocial_start\": \"2024-02-25 18:00:00\",\n",
    "    \"postsocial_end\": \"2024-03-02 13:00:00\",\n",
    "}\n",
    "key = {\"experiment_name\": exp[\"name\"]}\n",
    "\n",
    "# Define periods\n",
    "periods = {\n",
    "    \"presocial\": (exp[\"presocial_start\"], exp[\"presocial_end\"]),\n",
    "    \"social\": (exp[\"social_start\"], exp[\"social_end\"]),\n",
    "    \"postsocial\": (exp[\"postsocial_start\"], exp[\"postsocial_end\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch position data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_position_data(\n",
    "    key: Dict[str, str], period_start: str, period_end: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Loads position data (centroid tracking) for a specified time period.\n",
    "\n",
    "    Args:\n",
    "        key (dict): Key to identify experiment data (e.g., {\"experiment_name\": \"Exp1\"}).\n",
    "        period_start (str): Start datetime of the time period.\n",
    "        period_end (str): End datetime of the time period.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing position data for the specified period.\n",
    "                     Returns an empty DataFrame if no data found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Querying data from {period_start} to {period_end}...\")\n",
    "\n",
    "        # Create chunk restriction for the time period\n",
    "        chunk_restriction = acquisition.create_chunk_restriction(\n",
    "            key[\"experiment_name\"], period_start, period_end\n",
    "        )\n",
    "\n",
    "        # Fetch centroid tracking data for the specified period\n",
    "        centroid_df = (\n",
    "            streams.SpinnakerVideoSource * tracking.DenoisedTracking.Subject\n",
    "            & key\n",
    "            & {\"spinnaker_video_source_name\": \"CameraTop\"}\n",
    "            & chunk_restriction\n",
    "        ).fetch(format=\"frame\")\n",
    "\n",
    "        centroid_df = centroid_df.reset_index()\n",
    "        centroid_df = centroid_df.rename(\n",
    "            columns={\n",
    "                \"subject_name\": \"identity_name\",\n",
    "                \"timestamps\": \"time\",\n",
    "                \"subject_likelihood\": \"identity_likelihood\",\n",
    "            }\n",
    "        )\n",
    "        centroid_df = centroid_df.explode(\n",
    "            [\"time\", \"identity_likelihood\", \"x\", \"y\", \"likelihood\"]\n",
    "        )\n",
    "        centroid_df = centroid_df[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"experiment_name\",\n",
    "                \"identity_name\",\n",
    "                \"identity_likelihood\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"likelihood\",\n",
    "            ]\n",
    "        ].set_index(\"time\")\n",
    "\n",
    "        # Clean up the dataframe\n",
    "        if isinstance(centroid_df, pd.DataFrame) and not centroid_df.empty:\n",
    "            if \"spinnaker_video_source_name\" in centroid_df.columns:\n",
    "                centroid_df.drop(columns=[\"spinnaker_video_source_name\"], inplace=True)\n",
    "            print(f\"  Retrieved {len(centroid_df)} rows of position data\")\n",
    "        else:\n",
    "            print(\"  No data found for the specified period\")\n",
    "\n",
    "        return centroid_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"  Error loading position data for {key['experiment_name']} ({period_start} \"\n",
    "            f\"to {period_end}): {e}\"\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "position_data_list = []\n",
    "for period_name, (period_start, period_end) in periods.items():\n",
    "    # Load position data for this period\n",
    "    df = load_position_data(key, period_start, period_end)\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.assign(\n",
    "        experiment_name=exp[\"name\"], \n",
    "        period=period_name\n",
    "    )\n",
    "    position_data_list.append(df)\n",
    "\n",
    "position_df = concat_and_reorder(position_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_bouts(\n",
    "    pos_df: pd.DataFrame,\n",
    "    subject: str,\n",
    "    move_thresh: float = 4 * cm2px,  # cm -> px\n",
    "    max_speed: float = 100 * cm2px,  # cm/s -> px/s\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns sleep bouts for a given animal within the specified position data time period.\n",
    "\n",
    "    Args:\n",
    "        pos_df (pd.DataFrame): DataFrame containing position data.\n",
    "        subject (str): Name of the animal to filter by.\n",
    "        move_thresh (float): Movement (in px) threshold to define sleep bouts.\n",
    "        max_speed (float): Maximum speed threshold for excising swaps.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing sleep bouts for the specified animal.\n",
    "    \"\"\"\n",
    "    animal_data = pos_df[pos_df[\"identity_name\"] == subject].copy()\n",
    "    if animal_data.empty or not isinstance(animal_data, pd.DataFrame):\n",
    "        print(f\"No position data found for {subject}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Set some constants and placeholder `windows_df` which will be combined into `bouts_df`\n",
    "    sleep_win = pd.Timedelta(\"1m\")\n",
    "    sleep_windows_df = pd.DataFrame(\n",
    "        columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "    )\n",
    "\n",
    "    # Create time windows based on start and end time\n",
    "    data_start_time = animal_data.index.min()\n",
    "    data_end_time = animal_data.index.max()\n",
    "    window_starts = pd.date_range(\n",
    "        start=data_start_time, end=data_end_time, freq=sleep_win\n",
    "    )\n",
    "\n",
    "    # <s> Process each time window\n",
    "    period = animal_data[\"period\"].iloc[0]\n",
    "    pbar = tqdm(window_starts, desc=f\"Processing sleep bouts for {subject} in {period}\")\n",
    "    for win_start in pbar:\n",
    "        win_end = win_start + sleep_win\n",
    "        win_data = animal_data[\n",
    "            (animal_data.index >= win_start) & (animal_data.index < win_end)\n",
    "        ].copy()\n",
    "        if len(win_data) < 100:  # skip windows with too little data\n",
    "            continue\n",
    "\n",
    "        # Excise id swaps (based on pos / speed jumps)\n",
    "        # win_data = correct_swaps(win_data, max_speed)\n",
    "\n",
    "        # Calculate the displacement - maximum distance between any two points in the window\n",
    "        dx = win_data[\"x\"].max() - win_data[\"x\"].min()\n",
    "        dy = win_data[\"y\"].max() - win_data[\"y\"].min()\n",
    "        displacement = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "        # If displacement is less than threshold, consider it a sleep bout\n",
    "        if displacement < move_thresh:\n",
    "            new_bout = {\n",
    "                \"subject\": subject,\n",
    "                \"start\": win_start,\n",
    "                \"end\": win_end,\n",
    "                \"duration\": sleep_win,\n",
    "                \"period\": win_data[\"period\"].iloc[0],\n",
    "            }\n",
    "            sleep_windows_df = pd.concat(\n",
    "                [sleep_windows_df, pd.DataFrame([new_bout])], ignore_index=True\n",
    "            )\n",
    "    # </s>\n",
    "\n",
    "    # <s> Now merge consecutive sleep windows into continuous bouts\n",
    "    if sleep_windows_df.empty or not isinstance(sleep_windows_df, pd.DataFrame):\n",
    "        return pd.DataFrame(columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"])\n",
    "    # Initialize the merged bouts dataframe with the first window\n",
    "    sleep_bouts_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"subject\": subject,\n",
    "                \"start\": sleep_windows_df.iloc[0][\"start\"],\n",
    "                \"end\": sleep_windows_df.iloc[0][\"end\"],\n",
    "                \"duration\": sleep_windows_df.iloc[0][\"duration\"],\n",
    "                \"period\": sleep_windows_df.iloc[0][\"period\"],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Iterate through remaining windows and merge consecutive ones\n",
    "    for i in range(1, len(sleep_windows_df)):\n",
    "        current_window = sleep_windows_df.iloc[i]\n",
    "        last_bout = sleep_bouts_df.iloc[-1]\n",
    "\n",
    "        if current_window[\"start\"] == last_bout[\"end\"]:  # continue bout\n",
    "            sleep_bouts_df.at[len(sleep_bouts_df) - 1, \"end\"] = current_window[\"end\"]\n",
    "            sleep_bouts_df.at[len(sleep_bouts_df) - 1, \"duration\"] = (\n",
    "                sleep_bouts_df.iloc[-1][\"end\"] - sleep_bouts_df.iloc[-1][\"start\"]\n",
    "            )\n",
    "        else:  # start a new bout\n",
    "            new_bout = {\n",
    "                \"subject\": subject,\n",
    "                \"start\": current_window[\"start\"],\n",
    "                \"end\": current_window[\"end\"],\n",
    "                \"duration\": current_window[\"duration\"],\n",
    "                \"period\": current_window[\"period\"],\n",
    "            }\n",
    "            sleep_bouts_df = pd.concat(\n",
    "                [sleep_bouts_df, pd.DataFrame([new_bout])], ignore_index=True\n",
    "            )\n",
    "    # </s>\n",
    "\n",
    "    # Set min bout time\n",
    "    min_bout_time = pd.Timedelta(\"2m\")\n",
    "    sleep_bouts_df = sleep_bouts_df[sleep_bouts_df[\"duration\"] >= min_bout_time]\n",
    "\n",
    "    return sleep_bouts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save sleep bouts to parquet files for all experiments and periods.\"\"\"\n",
    "\n",
    "# For each experiment, for each period, load pos data, get sleep bouts, save to parquet\n",
    "\n",
    "pbar_exp = tqdm(experiments, desc=\"Processing experiments\")\n",
    "for exp in pbar_exp:\n",
    "    sleep_bouts_data_dict = {}\n",
    "    key = {\"experiment_name\": exp[\"name\"]}\n",
    "    sleep_bouts_data_dict[exp[\"name\"]] = {}\n",
    "    periods = {\n",
    "        \"presocial\": (exp[\"presocial_start\"], exp[\"presocial_end\"]),\n",
    "        \"social\": (exp[\"social_start\"], exp[\"social_end\"]),\n",
    "        \"postsocial\": (exp[\"postsocial_start\"], exp[\"postsocial_end\"]),\n",
    "    }\n",
    "    pbar_period = tqdm(periods.items(), desc=\"Processing periods\", leave=False)\n",
    "    for period_name, (period_start, period_end) in pbar_period:\n",
    "        print(f\"  Loading {period_name} period...\")\n",
    "        period_start = datetime.strptime(period_start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        period_end = datetime.strptime(period_end, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # load pos data for this period\n",
    "        pos_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period_name,\n",
    "            data_type=\"position\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "\n",
    "        # get sleep bouts for each subject\n",
    "        subjects = pos_df[\"identity_name\"].unique()\n",
    "        sleep_bouts_df = pd.DataFrame(\n",
    "            columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "        )\n",
    "        for subject in subjects:\n",
    "            subject_bouts = sleep_bouts(pos_df, subject)\n",
    "            if isinstance(subject_bouts, pd.DataFrame) and not subject_bouts.empty:\n",
    "                sleep_bouts_df = pd.concat(\n",
    "                    [sleep_bouts_df, subject_bouts], ignore_index=True\n",
    "                )\n",
    "\n",
    "        # save data dict\n",
    "        sleep_bouts_data_dict[exp[\"name\"]][period_name] = sleep_bouts_df\n",
    "        save_all_experiment_data(\n",
    "            experiments=[exp],\n",
    "            periods=[period_name],\n",
    "            data_dict=sleep_bouts_data_dict,\n",
    "            data_type=\"sleep\",\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "        print(f\"  Saved sleep bouts for {exp['name']} during {period_name} period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drink bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drink_bouts(\n",
    "    pos_df: pd.DataFrame,\n",
    "    subject: str,\n",
    "    spout_loc: tuple[float, float],  # x,y spout location in px\n",
    "    start_radius: float = 4 * 5.2,  # must be within X cm of spout, in px\n",
    "    move_thresh: float = 2.5 * 5.2,  # during bout must move less than X cm, in px\n",
    "    min_dur: float = 6,  # min duration of bout in seconds\n",
    "    max_dur: float = 90,  # max duration of bout in seconds\n",
    ") -> pd.DataFrame:  # cols: subject, start, end, duration, period\n",
    "    \"\"\"Returns drink bouts for a given animal within the specified position data time period.\"\"\"\n",
    "\n",
    "    animal_data = pos_df[pos_df[\"identity_name\"] == subject].copy()\n",
    "    if animal_data.empty or not isinstance(animal_data, pd.DataFrame):\n",
    "        print(f\"No position data found for {subject}\")\n",
    "        return pd.DataFrame(columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"])\n",
    "\n",
    "    # Smooth position data to 100ms intervals - only numeric columns\n",
    "    numeric_cols = animal_data.select_dtypes(include=[np.number]).columns\n",
    "    animal_data = animal_data[numeric_cols].resample(\"100ms\").mean().interpolate()\n",
    "    animal_data = animal_data.dropna()\n",
    "\n",
    "    # Add non-numeric columns back\n",
    "    animal_data[\"identity_name\"] = subject\n",
    "    animal_data[\"experiment_name\"] = pos_df[\"experiment_name\"].iloc[0]\n",
    "    animal_data[\"period\"] = pos_df[\"period\"].iloc[0]\n",
    "\n",
    "    # Calculate distance from spout\n",
    "    spout_x, spout_y = spout_loc\n",
    "    animal_data[\"dist_to_spout\"] = np.sqrt(\n",
    "        (animal_data[\"x\"] - spout_x) ** 2 + (animal_data[\"y\"] - spout_y) ** 2\n",
    "    )\n",
    "\n",
    "    # Find potential bout starts (within start_radius of spout)\n",
    "    near_spout = animal_data[\"dist_to_spout\"] <= start_radius\n",
    "\n",
    "    # Get period info\n",
    "    period = animal_data[\"period\"].iloc[0]\n",
    "\n",
    "    drink_bouts_df = pd.DataFrame(\n",
    "        columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "    )\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=len(animal_data), desc=f\"Processing drink bouts for {subject} in {period}\"\n",
    "    )\n",
    "    i = 0\n",
    "    while i < len(animal_data):\n",
    "        pbar.update(i - (i - 1))\n",
    "        # Skip if not near spout\n",
    "        if not near_spout.iloc[i]:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Found potential bout start\n",
    "        bout_start_time = animal_data.index[i]\n",
    "        bout_start_idx = i\n",
    "\n",
    "        # Track movement during potential bout\n",
    "        start_x = animal_data[\"x\"].iloc[i]\n",
    "        start_y = animal_data[\"y\"].iloc[i]\n",
    "\n",
    "        j = i\n",
    "        max_displacement = 0\n",
    "\n",
    "        # Continue while near spout and not moving too much\n",
    "        while j < len(animal_data):\n",
    "            current_time = animal_data.index[j]\n",
    "            elapsed_time = (current_time - bout_start_time).total_seconds()\n",
    "\n",
    "            # Calculate displacement from bout start position\n",
    "            current_x = animal_data[\"x\"].iloc[j]\n",
    "            current_y = animal_data[\"y\"].iloc[j]\n",
    "            displacement = np.sqrt(\n",
    "                (current_x - start_x) ** 2 + (current_y - start_y) ** 2\n",
    "            )\n",
    "            max_displacement = max(max_displacement, displacement)\n",
    "\n",
    "            # Check if bout should end\n",
    "            if max_displacement > move_thresh:\n",
    "                break\n",
    "\n",
    "            if elapsed_time > max_dur:\n",
    "                break\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        # Determine bout end\n",
    "        bout_end_time = (\n",
    "            animal_data.index[j - 1] if j > bout_start_idx else bout_start_time\n",
    "        )\n",
    "        bout_duration = (bout_end_time - bout_start_time).total_seconds()\n",
    "\n",
    "        # Check if bout meets duration criteria\n",
    "        if min_dur < bout_duration < max_dur:\n",
    "            new_bout = {\n",
    "                \"subject\": subject,\n",
    "                \"start\": bout_start_time,\n",
    "                \"end\": bout_end_time,\n",
    "                \"duration\": pd.Timedelta(seconds=bout_duration),\n",
    "                \"period\": period,\n",
    "            }\n",
    "            drink_bouts_df = pd.concat(\n",
    "                [drink_bouts_df, pd.DataFrame([new_bout])], ignore_index=True\n",
    "            )\n",
    "\n",
    "        # Move to next potential bout (skip past current bout end)\n",
    "        i = max(j, i + 1)\n",
    "\n",
    "    pbar.close()\n",
    "    return drink_bouts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save drink bouts to parquet files for all experiments and periods.\"\"\"\n",
    "\n",
    "# For each experiment, for each period, load pos data, get drink bouts, save to parquet\n",
    "\n",
    "pbar_exp = tqdm(experiments, desc=\"Processing experiments\")\n",
    "for exp in pbar_exp:\n",
    "    drink_bouts_data_dict = {}\n",
    "    key = {\"experiment_name\": exp[\"name\"]}\n",
    "    drink_bouts_data_dict[exp[\"name\"]] = {}\n",
    "    pbar_period = tqdm(periods, desc=\"Processing periods\", leave=False)\n",
    "    for period_name in pbar_period:\n",
    "        print(f\"  Loading {period_name} period...\")\n",
    "\n",
    "        # load pos data for this period\n",
    "        pos_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period_name,\n",
    "            data_type=\"position\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "\n",
    "        # get drink bouts for each subject\n",
    "        subjects = pos_df[\"identity_name\"].unique()\n",
    "        drink_bouts_df = pd.DataFrame(\n",
    "            columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "        )\n",
    "        for subject in subjects:\n",
    "            spout_loc = (1280, 500) if \"aeon3\" in exp[\"name\"] else (1245, 535)\n",
    "            subject_bouts = drink_bouts(pos_df, subject, spout_loc)\n",
    "            if isinstance(subject_bouts, pd.DataFrame) and not subject_bouts.empty:\n",
    "                drink_bouts_df = pd.concat(\n",
    "                    [drink_bouts_df, subject_bouts], ignore_index=True\n",
    "                )\n",
    "\n",
    "        # save data dict\n",
    "        drink_bouts_data_dict[exp[\"name\"]][period_name] = drink_bouts_df\n",
    "        save_all_experiment_data(\n",
    "            experiments=[exp],\n",
    "            periods=[period_name],\n",
    "            data_dict=drink_bouts_data_dict,\n",
    "            data_type=\"drink\",\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "        print(f\"  Saved drink bouts for {exp['name']} during {period_name} period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_bouts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given pos_df, animal name, nest xy, reutrn all explore bouts in df\n",
    "\n",
    "nest_center = np.array((1215, 530))\n",
    "cm2px = 5.2\n",
    "nest_radius = 14 * cm2px  # 14 cm, in px\n",
    "\n",
    "\n",
    "def explore_bouts(\n",
    "    pos_df: pd.DataFrame,\n",
    "    subject: str,\n",
    "    nest_center: np.ndarray,\n",
    "    nest_radius: float = 14 * 5.2,  # 14 cm, in px\n",
    "    max_speed: float = 100 * 5.2,  # 100 cm/s, in px/s\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns exploration bouts for a given animal within the specified position data time period.\n",
    "\n",
    "    Args:\n",
    "        pos_df (pd.DataFrame): DataFrame containing position data.\n",
    "        subject (str): Name of the animal to filter by.\n",
    "        nest_center (np.ndarray): Coordinates of the nest center.\n",
    "        nest_radius (float): Radius of the nest area (default: 14 cm in px).\n",
    "        max_speed (float): Maximum speed threshold for excising swaps (default: 100 cm/s in px/s).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing exploration bouts for the specified animal.\n",
    "    \"\"\"\n",
    "    animal_data = pos_df[pos_df[\"identity_name\"] == subject].copy()\n",
    "    if animal_data.empty or not isinstance(animal_data, pd.DataFrame):\n",
    "        print(f\"No position data found for {subject}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Set some constants and placeholder `windows_df` which will be combined into `bouts_df`\n",
    "    explore_win = pd.Timedelta(\"1m\")\n",
    "    explore_windows_df = pd.DataFrame(\n",
    "        columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "    )\n",
    "\n",
    "    # Create time windows based on start and end time\n",
    "    data_start_time = animal_data.index.min()\n",
    "    data_end_time = animal_data.index.max()\n",
    "    window_starts = pd.date_range(\n",
    "        start=data_start_time, end=data_end_time, freq=explore_win\n",
    "    )\n",
    "\n",
    "    # <s> Process each time window (use tqdm for progress bar)\n",
    "    period = animal_data[\"period\"].iloc[0]\n",
    "    pbar = tqdm(window_starts, desc=f\"Processing explore bouts for {subject} in {period}\")\n",
    "    for win_start in pbar:\n",
    "        win_end = win_start + explore_win\n",
    "        win_data = animal_data[\n",
    "            (animal_data.index >= win_start) & (animal_data.index < win_end)\n",
    "        ].copy()\n",
    "        if len(win_data) < 100:  # skip windows with too little data\n",
    "            continue\n",
    "\n",
    "        # Excise id swaps (based on pos / speed jumps)\n",
    "        win_data = correct_swaps(win_data, max_speed)\n",
    "\n",
    "        # If majority of time in a window is outside nest, consider it an explore bout\n",
    "        dx = win_data[\"x\"] - nest_center[0]\n",
    "        dy = win_data[\"y\"] - nest_center[1]\n",
    "        distance_from_nest = np.sqrt(dx**2 + dy**2)\n",
    "        frac_out_nest = (distance_from_nest > nest_radius).sum() / len(win_data)\n",
    "        if frac_out_nest > 0.5:\n",
    "            new_bout = {\n",
    "                \"subject\": subject,\n",
    "                \"start\": win_start,\n",
    "                \"end\": win_end,\n",
    "                \"duration\": explore_win,\n",
    "                \"period\": win_data[\"period\"].iloc[0],\n",
    "            }\n",
    "            explore_windows_df = pd.concat(\n",
    "                [explore_windows_df, pd.DataFrame([new_bout])], ignore_index=True\n",
    "            )\n",
    "    # </s>\n",
    "\n",
    "    # <s> Now merge consecutive explore windows into continuous bouts\n",
    "    if explore_windows_df.empty or not isinstance(explore_windows_df, pd.DataFrame):\n",
    "        return pd.DataFrame(columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"])\n",
    "    # Initialize the merged bouts dataframe with the first window\n",
    "    explore_bouts_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"subject\": subject,\n",
    "                \"start\": explore_windows_df.iloc[0][\"start\"],\n",
    "                \"end\": explore_windows_df.iloc[0][\"end\"],\n",
    "                \"duration\": explore_windows_df.iloc[0][\"duration\"],\n",
    "                \"period\": explore_windows_df.iloc[0][\"period\"],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Iterate through remaining windows and merge consecutive ones\n",
    "    for i in range(1, len(explore_windows_df)):\n",
    "        current_window = explore_windows_df.iloc[i]\n",
    "        last_bout = explore_bouts_df.iloc[-1]\n",
    "\n",
    "        if current_window[\"start\"] == last_bout[\"end\"]:  # continue bout\n",
    "            explore_bouts_df.at[len(explore_bouts_df) - 1, \"end\"] = current_window[\"end\"]\n",
    "            explore_bouts_df.at[len(explore_bouts_df) - 1, \"duration\"] = (\n",
    "                explore_bouts_df.iloc[-1][\"end\"] - explore_bouts_df.iloc[-1][\"start\"]\n",
    "            )\n",
    "        else:  # start a new bout\n",
    "            new_bout = {\n",
    "                \"subject\": subject,\n",
    "                \"start\": current_window[\"start\"],\n",
    "                \"end\": current_window[\"end\"],\n",
    "                \"duration\": current_window[\"duration\"],\n",
    "                \"period\": current_window[\"period\"],\n",
    "            }\n",
    "            explore_bouts_df = pd.concat(\n",
    "                [explore_bouts_df, pd.DataFrame([new_bout])], ignore_index=True\n",
    "            )\n",
    "    # </s>\n",
    "\n",
    "    return explore_bouts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save explore bouts to parquet files for each experiment and period\"\"\"\n",
    "\n",
    "# For each experiment, for each period, load pos data, get explore bouts, save to parquet\n",
    "\n",
    "pbar_exp = tqdm(experiments, desc=\"Processing experiments\")\n",
    "for exp in pbar_exp:\n",
    "    sleep_bouts_data_dict = {}\n",
    "    key = {\"experiment_name\": exp[\"name\"]}\n",
    "\n",
    "    # get nest center for this exp\n",
    "    epoch_query = acquisition.Epoch & (acquisition.Chunk & key).proj(\"epoch_start\")\n",
    "    active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query\n",
    "    roi_locs = dict(\n",
    "        zip(*active_region_query.fetch(\"region_name\", \"region_data\"), strict=True)\n",
    "    )\n",
    "    points = roi_locs[\"NestRegion\"][\"ArrayOfPoint\"]\n",
    "    vertices = np.array([[float(point[\"X\"]), float(point[\"Y\"])] for point in points])\n",
    "    nest_center = np.mean(vertices, axis=0)\n",
    "\n",
    "    sleep_bouts_data_dict[exp[\"name\"]] = {}\n",
    "    periods = {\n",
    "        \"presocial\": (exp[\"presocial_start\"], exp[\"presocial_end\"]),\n",
    "        \"social\": (exp[\"social_start\"], exp[\"social_end\"]),\n",
    "        \"postsocial\": (exp[\"postsocial_start\"], exp[\"postsocial_end\"]),\n",
    "    }\n",
    "    pbar_period = tqdm(periods.items(), desc=\"Processing periods\", leave=False)\n",
    "    for period_name, (period_start, period_end) in pbar_period:\n",
    "        print(f\"  Loading {period_name} period...\")\n",
    "        period_start = datetime.strptime(period_start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        period_end = datetime.strptime(period_end, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # load pos data for this period\n",
    "        pos_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period_name,\n",
    "            data_type=\"position\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "\n",
    "        # get explore bouts for each subject\n",
    "        subjects = pos_df[\"identity_name\"].unique()\n",
    "        sleep_bouts_df = pd.DataFrame(\n",
    "            columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\"]\n",
    "        )\n",
    "        for subject in subjects:\n",
    "            subject_bouts = explore_bouts(pos_df, subject, nest_center)\n",
    "            if isinstance(subject_bouts, pd.DataFrame) and not subject_bouts.empty:\n",
    "                sleep_bouts_df = pd.concat(\n",
    "                    [sleep_bouts_df, subject_bouts], ignore_index=True\n",
    "                )\n",
    "\n",
    "        # save data dict\n",
    "        sleep_bouts_data_dict[exp[\"name\"]][period_name] = sleep_bouts_df\n",
    "        save_all_experiment_data(\n",
    "            experiments=[exp],\n",
    "            periods=[period_name],\n",
    "            data_dict=sleep_bouts_data_dict,\n",
    "            data_type=\"explore\",\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "        print(f\"  Saved explore bouts for {exp['name']} during {period_name} period.\")\n",
    "\n",
    "\n",
    "key = {\"experiment_name\": \"social0.2-aeon3\"}\n",
    "epoch_query = acquisition.Epoch & (acquisition.Chunk & key).proj(\"epoch_start\")\n",
    "active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query\n",
    "roi_locs = dict(\n",
    "    zip(*active_region_query.fetch(\"region_name\", \"region_data\"), strict=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example usage:\"\"\"\n",
    "\n",
    "explore_df = load_data_from_parquet(\n",
    "    experiment_name=\"social0.2-aeon3\",\n",
    "    period=\"presocial\",\n",
    "    data_type=\"explore\",\n",
    "    data_dir=data_dir,\n",
    "    set_time_index=True,\n",
    ")\n",
    "display(explore_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
